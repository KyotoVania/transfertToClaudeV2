Guide Correctif pour l'Ingénierie d'un Moteur Rythmique de Pointe pour AuraSync




Introduction : Validation du Défi et Définition de la Voie à Suivre


Le rapport d'analyse initial du projet AuraSync démontre une compréhension approfondie des forces et des faiblesses de son moteur d'analyse audio.1 L'identification du système de détection de BPM comme le principal goulot d'étranglement est non seulement correcte, mais elle met en lumière une limitation architecturale fondamentale commune à de nombreux systèmes d'analyse en temps réel. Le code existant, notamment le hook
useAudioAnalyzer.ts et la classe BPMDetector.ts, révèle une implémentation compétente et bien structurée d'un détecteur de tempo de première génération.1
Le problème principal ne réside pas dans des erreurs de programmation, mais dans les limites inhérentes aux algorithmes choisis. Le système actuel est un exemple classique de détecteur de tempo de base, qui, bien que fonctionnel, est incapable de gérer la complexité et l'ambiguïté de la musique du monde réel. Ce guide a pour objectif de fournir une feuille de route technique et conceptuelle pour faire évoluer ce système vers ce que l'on pourrait appeler un moteur rythmique de troisième génération : un système qui ne se contente pas d'estimer un tempo, mais qui suit activement le battement avec une intelligence musicale.
Pour ce faire, ce guide de correction est structuré en un parcours de mise à niveau logique et incrémental, divisé en trois parties distinctes. Chaque partie s'appuie sur la précédente pour construire un système de plus en plus sophistiqué et robuste.
1. Partie I : Forger un Signal Propre. La première étape, et la plus cruciale, consiste à corriger la source de données qui alimente l'ensemble du système : la Fonction de Détection d'Onset (ODF). Nous allons diagnostiquer les faiblesses de l'approche actuelle et la remplacer par une méthode robuste et perceptuellement pertinente, basée sur les meilleures pratiques de la recherche en Music Information Retrieval (MIR).
2. Partie II : Résoudre l'Ambigüité. Une fois que nous disposons d'un signal ODF de haute qualité, nous nous attaquerons à la limitation principale du détecteur de tempo actuel : son incapacité à gérer l'ambiguïté. Nous remplacerons le système à réponse unique par un moteur multi-hypothèses capable d'analyser et de résoudre la complexité rythmique.
3. Partie III : Atteindre la Synchronisation. L'étape finale est le saut qualitatif le plus important. Nous passerons de la simple estimation du tempo au suivi de battement (beat tracking). En implémentant un algorithme de programmation dynamique, nous saurons non seulement à quelle vitesse les battements se produisent, mais aussi précisément où ils se trouvent dans le temps, ce qui est la clé d'une synchronisation visuelle de niveau professionnel.
En suivant ce parcours, AuraSync passera d'un visualiseur qui réagit à l'énergie audio à un système qui comprend et anticipe la structure temporelle de la musique, ouvrant ainsi un champ de possibilités créatives beaucoup plus vaste.
________________


Partie I : Correction Fondamentale — Forger une Fonction de Détection d'Onset (ODF) Robuste


La qualité de la Fonction de Détection d'Onset (ODF) est le facteur le plus déterminant pour la performance de l'ensemble du système de détection rythmique.1 Une ODF de mauvaise qualité, ou "bruitée", fournira des informations trompeuses aux algorithmes en aval, conduisant inévitablement à des estimations de tempo erronées et instables. La première étape corrective consiste donc à s'assurer que le "carburant" du moteur est aussi propre et fiable que possible.


1.1 Analyse Diagnostique : Le Problème du "Carburant Bruité"


L'analyse du code source révèle que l'ODF est actuellement générée au sein de la fonction calculateSpectralFeatures dans le fichier useAudioAnalyzer.ts.1 L'extrait de code pertinent est le suivant 1 :


TypeScript




// Onset Detection Function (ODF) basée sur le flux spectral médian
const spectralChanges: number =; 

for (let i = 1; i < frequencies.length - 1; i++) {
 const magnitude = frequencies[i] / 255; 
 //... autres calculs...
 const prevMag = prevFrequenciesRef.current[i]; 
 const change = magnitude - prevMag; 

 // Redressement demi-onde : changements positifs uniquement
 if (change > 0) {
   spectralChanges.push(change);
 }
}

const flux = calculateMedian(spectralChanges); // Le flux spectral est la médiane des changements

Cette implémentation calcule le flux spectral en agrégeant les augmentations d'énergie sur l'ensemble du spectre de fréquences (on parle d'ODF "pleine bande" ou "full-band") et en utilisant la médiane pour obtenir une valeur unique.1 Bien que l'utilisation de la médiane soit une technique de moyennage robuste, l'approche pleine bande est la source principale de la non-fiabilité du système.
Le problème peut être illustré par le scénario "cymbale contre grosse caisse". Dans de nombreux genres musicaux, le rythme principal est porté par des instruments percussifs dans les basses et moyennes fréquences (grosse caisse, caisse claire). Cependant, des événements comme un crash de cymbale, bien que moins importants rythmiquement, libèrent une énorme quantité d'énergie dans les hautes fréquences. Dans une approche pleine bande, cette explosion d'énergie dans les aigus peut créer un pic dans l'ODF bien plus important que celui généré par la grosse caisse. Le système perçoit alors la cymbale comme l'événement le plus saillant, même si elle ne correspond pas au battement principal que l'auditeur taperait du pied. Cela introduit un bruit significatif dans le signal que le BPMDetector doit analyser, le rendant "vulnérable à la domination par des événements musicaux très forts mais non rythmiques".1
Il existe une chaîne de causalité directe qui mène de cette ODF non robuste aux problèmes de détection de BPM observés :
1. La fonction calculateSpectralFeatures génère une ODF "pleine bande" en agrégeant les changements sur tout le spectre.1
2. Cette ODF est "polluée" par des événements non rythmiques mais énergétiques (voix, cymbales, guitares saturées), comme le critique le rapport d'analyse.1
3. Ce signal ODF bruité est ensuite transmis à l'instance de BPMDetector.
4. La fonction autocorrelation au cœur de BPMDetector cherche des motifs récurrents.1 Si les pics les plus importants de l'ODF ne correspondent pas au rythme réel, l'ACF ne trouvera pas de corrélation forte et claire, ou pire, trouvera une corrélation pour une périodicité erronée.
5. Cela se manifeste directement par les "erreurs d'octave" (par exemple, 80 BPM au lieu de 160) et la faible confiance de détection, qui sont les faiblesses majeures identifiées dans l'analyse de BPMDetector.ts.1
En conclusion, la non-fiabilité du BPMDetector n'est pas une défaillance isolée de l'algorithme d'autocorrélation lui-même, mais une conséquence directe et inévitable du fait qu'il est alimenté par une ODF de mauvaise qualité. La correction de l'ODF est donc l'amélioration prioritaire avec le plus grand impact, qui débloquera le potentiel de toutes les étapes ultérieures.


1.2 Guide d'Implémentation : L'ODF Multi-Bandes et Pondérée Perceptuellement


Pour corriger ce problème fondamental, nous allons remplacer la logique actuelle par une approche multi-bandes, en s'inspirant directement des recommandations de la littérature scientifique.1 Cette méthode divise le spectre en plusieurs bandes qui correspondent mieux à la perception auditive humaine avant de calculer le flux, empêchant ainsi une seule bande de dominer le signal global.


Étape 1 : Implémenter une Banque de Filtres Mel


La première étape consiste à créer une banque de filtres triangulaires espacés selon l'échelle Mel. L'échelle Mel est une échelle de hauteur tonale perceptuelle qui modélise la façon dont les humains perçoivent les différences de hauteur. Cela nous permet de nous concentrer sur les bandes de fréquences les plus pertinentes pour la perception du rythme.
Voici une fonction TypeScript pure pour générer cette banque de filtres. Elle ne nécessite aucune dépendance externe.


TypeScript




/**
* Crée une banque de filtres triangulaires espacés sur l'échelle Mel.
* @param fftSize La taille de la FFT (par ex. 2048).
* @param melBands Le nombre de bandes Mel à créer (par ex. 40).
* @param sampleRate La fréquence d'échantillonnage de l'audio (par ex. 44100).
* @returns Une matrice de filtres.
*/
function createMelFilterbank(fftSize: number, melBands: number, sampleRate: number): number {
   const toMel = (hz: number): number => 1127 * Math.log(1 + hz / 700);
   const toHz = (mel: number): number => 700 * (Math.exp(mel / 1127) - 1);

   const maxMel = toMel(sampleRate / 2);
   const minMel = toMel(30); // Fréquence minimale
   const melStep = (maxMel - minMel) / (melBands + 1);

   const melCenters: number =;
   for (let i = 0; i < melBands + 2; i++) {
       melCenters.push(minMel + i * melStep);
   }

   const hzPoints = melCenters.map(toHz);
   const fftBinPoints = hzPoints.map(hz => Math.floor((fftSize + 1) * hz / sampleRate));

   const filterbank: number =;
   for (let i = 0; i < melBands; i++) {
       const filter = new Array(fftSize / 2 + 1).fill(0);
       const start = fftBinPoints[i];
       const center = fftBinPoints[i + 1];
       const end = fftBinPoints[i + 2];

       for (let j = start; j < center; j++) {
           filter[j] = (j - start) / (center - start);
       }
       for (let j = center; j < end; j++) {
           filter[j] = (end - j) / (end - center);
       }
       filterbank.push(filter);
   }
   return filterbank;
}



Étape 2 : Calculer le Flux Spectral par Bande et Agréger


Nous allons maintenant créer une nouvelle fonction, calculateRobustODF, qui remplacera la logique de calcul du flux dans calculateSpectralFeatures. Cette fonction utilisera la banque de filtres Mel, calculera le flux pour chaque bande indépendamment, puis agrégera les résultats en utilisant la médiane, une technique robuste proposée par McFee et Ellis.1
Cette fonction devra être gérée au sein du hook useAudioAnalyzer, qui devra conserver l'état des énergies Mel précédentes (prevMelEnergies) et la banque de filtres (melFilterbank).


TypeScript




// À ajouter dans useAudioAnalyzer.ts, en dehors du composant
// Pseudo-code basé sur la feuille de route de [1], page 15, mais complet et commenté.

/**
* Calcule une Fonction de Détection d'Onset (ODF) robuste en utilisant une approche multi-bandes.
* @param fftMagnitudes Le tableau des magnitudes de la FFT pour la trame actuelle.
* @param prevMelEnergies Le tableau des énergies Mel de la trame précédente.
* @param melFilterbank La matrice de la banque de filtres Mel.
* @param melBands Le nombre de bandes Mel.
* @returns La valeur de l'ODF pour la trame courante.
*/
function calculateRobustODF(
   fftMagnitudes: Uint8Array, 
   prevMelEnergies: Float32Array,
   melFilterbank: number,
   melBands: number
): number {
   const melEnergies = new Float32Array(melBands).fill(0);

   // 1. Appliquer la banque de filtres pour obtenir les énergies par bande Mel
   for (let i = 0; i < melBands; i++) {
       for (let j = 0; j < fftMagnitudes.length; j++) {
           // Normaliser la magnitude de la FFT (0-255) en 0-1
           const normalizedMagnitude = fftMagnitudes[j] / 255;
           melEnergies[i] += melFilterbank[i][j] * normalizedMagnitude;
       }
   }

   // 2. Calculer le flux spectral (différence positive) pour chaque bande
   const bandFluxes: number =;
   for (let i = 0; i < melBands; i++) {
       const flux = melEnergies[i] - prevMelEnergies[i];
       // Redressement demi-onde : ne conserver que les augmentations d'énergie
       if (flux > 0) {
           bandFluxes.push(flux);
       }
   }

   // 3. Mettre à jour l'historique des énergies Mel pour la prochaine trame
   prevMelEnergies.set(melEnergies);

   // 4. Agréger les flux de chaque bande avec la médiane pour une robustesse maximale
   // La médiane garantit qu'un pic doit se produire dans plus de la moitié des bandes
   // pour être reflété dans l'ODF finale.
   if (bandFluxes.length === 0) return 0;
   
   const sortedFluxes = bandFluxes.sort((a, b) => a - b);
   const mid = Math.floor(sortedFluxes.length / 2);
   const medianFlux = sortedFluxes.length % 2!== 0 
      ? sortedFluxes[mid] 
       : (sortedFluxes[mid - 1] + sortedFluxes[mid]) / 2;
   
   // Retourner la valeur de l'ODF, potentiellement mise à l'échelle pour une meilleure plage dynamique
   return medianFlux;
}

L'intégration de cette approche constitue la première et la plus importante correction. Elle fournira un signal d'entrée propre et fiable pour les étapes suivantes, améliorant considérablement la robustesse de l'ensemble du système pour un large éventail de genres musicaux.1
________________


Partie II : Périodicité Avancée — Résoudre l'Ambigüité du Tempo


Avec une ODF robuste en place, nous pouvons maintenant nous attaquer à la deuxième faiblesse majeure du système actuel : son moteur de périodicité. Le BPMDetector.ts actuel, bien qu'étant une implémentation correcte de l'autocorrélation, est fondamentalement limité par sa nature à réponse unique et sa vulnérabilité aux erreurs d'octave.


2.1 Analyse Diagnostique : Le Dilemme "80 contre 160 BPM"


Le cœur de BPMDetector.ts est la fonction autocorrelation.1 L'autocorrélation (ACF) est un outil mathématique qui mesure la similarité d'un signal avec une version décalée de lui-même. Un pic élevé dans l'ACF pour un certain décalage (ou
lag) indique une forte périodicité à cet intervalle.
Le problème, comme souligné dans le rapport d'analyse, est que le tempo est une perception psychoacoustique, pas une simple mesure mathématique.1 Un morceau à 160 BPM a une pulsation forte toutes les 0.375 secondes. Cependant, il a aussi une pulsation secondaire très régulière toutes les 0.75 secondes (correspondant à 80 BPM). Pour une fonction mathématique comme l'ACF, ces deux périodicités peuvent générer des pics de corrélation très forts. Le code actuel cherche simplement le pic le plus élevé dans une plage de tempos plausibles (70-190 BPM) et le retourne comme la seule réponse.1 Il n'a "aucun mécanisme pour résoudre cette ambiguïté", ce qui conduit à des erreurs d'octave fréquentes.1
De plus, cette approche à "hypothèse unique" bride le potentiel du visualiseur. La musique contient souvent plusieurs couches rythmiques simultanées et valides. Un système avancé devrait pouvoir exposer ces différentes hypothèses de tempo, chacune avec sa propre confiance, plutôt que de forcer une seule valeur qui peut être erronée.1


2.2 La Solution Hybride : Tempogramme DFT avec Validation par ACF


Pour surmonter ces limitations, nous allons adopter une approche hybride qui combine les forces de deux algorithmes différents, comme le suggère l'analyse comparative de la littérature.1 Nous utiliserons la Transformée de Fourier Discrète (DFT) pour générer plusieurs hypothèses de tempo, puis nous utiliserons l'autocorrélation (ACF) existante pour valider ces hypothèses et résoudre les ambiguïtés.


La Nécessité d'une Implémentation FFT Personnalisée


Une étape cruciale et non triviale doit être abordée ici. La solution proposée implique de calculer un "tempogramme" en appliquant une DFT à l'historique de l'ODF.1
1. Le tempogramme est une représentation temps-fréquence du rythme. Il est obtenu en calculant la DFT d'une fenêtre glissante de l'ODF. Dans ce contexte, les "fréquences" de la DFT correspondent à des tempos (en BPM).
2. L'API Web Audio fournit une méthode getByteFrequencyData qui est une Transformée de Fourier Rapide (FFT, un algorithme efficace pour calculer la DFT) du signal audio brut.
3. Cependant, notre besoin est de calculer la FFT non pas de l'audio, mais de notre tampon d'historique de l'ODF (odfHistoryRef.current), qui est un signal distinct que nous construisons nous-mêmes dans le domaine temporel.
4. L'API Web Audio ne fournit aucun moyen d'exécuter une FFT sur un tableau JavaScript arbitraire (comme un Float32Array) depuis le thread principal.
Par conséquent, pour respecter la contrainte de ne pas utiliser de bibliothèques externes, il est nécessaire d'implémenter notre propre fonction FFT en TypeScript. Heureusement, l'algorithme Radix-2, bien que complexe, est bien documenté et peut être implémenté de manière récursive sans dépendances.3
Voici une implémentation de base d'une FFT récursive en TypeScript qui peut être utilisée. Elle prend en entrée un tableau de nombres réels (notre ODF) et retourne un tableau de nombres complexes représentant le spectre.


TypeScript




// Fichier : utils/fft.ts (nouveau fichier)

// Type pour représenter un nombre complexe
type Complex = { re: number; im: number };

/**
* Calcule la Transformée de Fourier Discrète (DFT) en utilisant l'algorithme
* récursif de la Transformée de Fourier Rapide (FFT) de Cooley-Tukey.
* @param inputArray Un tableau de nombres réels. La longueur doit être une puissance de 2.
* @returns Un tableau de nombres complexes représentant le spectre de fréquence.
*/
export function fft(inputArray: number): Complex {
   const N = inputArray.length;

   if (N <= 1) {
       return [{ re: inputArray |

| 0, im: 0 }];
   }

   // La longueur du tableau doit être une puissance de 2.
   if (N % 2!== 0) {
       throw new Error("La taille du tableau d'entrée doit être une puissance de 2 pour cet algorithme FFT.");
   }

   // Diviser le tableau en parties paires et impaires
   const evens = inputArray.filter((_, index) => index % 2 === 0);
   const odds = inputArray.filter((_, index) => index % 2!== 1);

   const fftEvens = fft(evens);
   const fftOdds = fft(odds);

   const result: Complex = new Array(N);

   for (let k = 0; k < N / 2; k++) {
       const angle = -2 * Math.PI * k / N;
       const t: Complex = {
           re: Math.cos(angle) * fftOdds[k].re - Math.sin(angle) * fftOdds[k].im,
           im: Math.sin(angle) * fftOdds[k].re + Math.cos(angle) * fftOdds[k].im,
       };

       result[k] = {
           re: fftEvens[k].re + t.re,
           im: fftEvens[k].im + t.im,
       };
       result[k + N / 2] = {
           re: fftEvens[k].re - t.re,
           im: fftEvens[k].im - t.im,
       };
   }

   return result;
}



Guide d'Implémentation de l'Induction de Tempo Hybride


1. Maintenir un Historique de l'ODF : Dans useAudioAnalyzer.ts, continuez à pousser les valeurs de la nouvelle calculateRobustODF dans un tampon circulaire (odfHistoryRef.current). Une taille de 256 ou 512 échantillons est un bon point de départ, correspondant à 5-10 secondes de données à une fréquence d'échantillonnage de l'ODF de 50-100 Hz.
2. Calculer le Tempogramme DFT : À chaque trame (ou à une fréquence moins élevée pour des raisons de performance), exécutez la fonction fft sur l'historique de l'ODF. Calculez la magnitude de chaque composante complexe (magnitude=re2+im2​). Chaque indice k du tableau de sortie de la FFT correspond à une fréquence rythmique fk​=k⋅(ODF_SAMPLE_RATE/N), où N est la taille de la FFT. Cette fréquence peut être convertie en BPM : BPMk​=fk​⋅60. Identifiez les pics dans ce spectre de magnitude pour obtenir une liste de tempos candidats et leur force.
3. Valider avec l'ACF : C'est ici que le système devient intelligent. Pour chaque tempo candidat identifié par la DFT, utilisez l'algorithme d'autocorrélation existant pour valider sa plausibilité.
   * Calculez l'ACF de l'historique de l'ODF (la fonction autocorrelation de BPMDetector.ts peut être réutilisée).
   * Pour un candidat BPM, calculez le lag correspondant : lag=(60/BPM)⋅ODF_SAMPLE_RATE.
   * Vérifiez la force du pic de l'ACF à ce lag précis, ainsi qu'à ses multiples et sous-multiples (par exemple, lag/2, 2⋅lag).
   * Logique de décision pour les erreurs d'octave : Si la DFT suggère des pics à 80 BPM et 160 BPM, comparez la force des pics correspondants dans l'ACF. Souvent, le tempo correct aura un pic plus "propre" (plus proéminent par rapport à son voisinage) dans l'ACF, même s'il n'est pas le plus élevé en valeur absolue. Une heuristique simple consiste à favoriser le tempo dont le pic ACF est le plus fort par rapport à ses propres harmoniques.
Cette approche hybride répond directement aux deux limitations majeures du système actuel : elle produit de multiples hypothèses de tempo et dispose d'un mécanisme robuste pour résoudre les ambiguïtés d'octave.
Le tableau suivant, adapté du rapport d'analyse 1, justifie le choix de cette approche hybride en comparant les différentes méthodes de détection de périodicité.


Caractéristique
	Fonction d'Autocorrélation (ACF)
	Banque de Filtres en Peigne
	DFT (Tempogramme)
	Histogramme d'IOI
	Principe de base
	Auto-similarité temporelle
	Résonance temporelle
	Analyse de fréquence
	Distribution statistique
	Avantages
	Simple, rapide, efficace
	Causal, temps réel, spectre de tempo
	Gère bien les hypothèses multiples
	Robuste pour les signaux percussifs
	Inconvénients
	Erreurs d'octave, hypothèse unique
	Moins précis pour les tempos variables
	Nécessite une fenêtre, résolution limitée
	Dépendant de la détection d'onsets
	Coût de calcul
	Faible
	Faible à modéré
	Modéré
	Modéré
	Références clés
	1
	1
	1
	1
	Ce tableau montre clairement que si l'ACF est rapide, elle souffre de limitations critiques. La DFT, en revanche, excelle dans la gestion de multiples hypothèses. En les combinant, nous obtenons le meilleur des deux mondes : la capacité de la DFT à générer des candidats et la capacité de l'ACF à les valider et à les affiner.
________________


Partie III : L'Étalon-Or — Suivi de Battement par Programmation Dynamique


Les deux premières parties ont permis de construire un système capable d'estimer de manière robuste un ou plusieurs tempos globaux. Cependant, pour atteindre un niveau de synchronisation visuelle véritablement professionnel, il ne suffit pas de savoir à quelle vitesse les battements se produisent ; il faut savoir précisément où ils se trouvent sur la ligne de temps. C'est la distinction fondamentale entre l'estimation du tempo et le suivi de battement (beat tracking).1 Cette dernière partie détaille l'implémentation de l'algorithme de référence pour cette tâche : le suivi de battement par programmation dynamique.


3.1 Le Saut Conceptuel : De "Quelle Vitesse?" à "Quel Instant?"


Le système actuel tente d'estimer la phase du battement (beatPhase) en se basant sur une heuristique simple : la réinitialisation d'un chronomètre lors de la détection d'un transitoire global.1 Comme le souligne le rapport d'analyse, cette méthode est "sujette à la dérive" et "ne constitue pas un véritable suivi de battement".1 Elle échoue car tous les onsets ne sont pas des battements (par exemple, les syncopes) et tous les battements n'ont pas d'onset (par exemple, les silences).
L'algorithme de suivi de battement par programmation dynamique, popularisé par Daniel P.W. Ellis en 2007, reformule le problème.5 Il ne cherche pas les pics individuellement, mais la
séquence de battements la plus optimale à travers le temps. Une séquence est considérée comme optimale si elle maximise un score qui équilibre deux contraintes souvent contradictoires :
1. Correspondance locale : Les battements doivent coïncider avec des moments de forte énergie d'onset.
2. Régularité rythmique : Les battements doivent être espacés de manière relativement régulière, conformément à un tempo cible.
L'implémentation de cet algorithme permet d'unifier trois problèmes distincts qui sont actuellement gérés par des logiques séparées et fragiles dans AuraSync. La sortie de l'algorithme est une liste unique et cohérente d'horodatages de battements. À partir de cette seule source de vérité, on peut dériver :
* Le primaryBPM : en calculant la médiane des intervalles entre les horodatages.
* Le isBeat : en retournant true uniquement sur les trames qui correspondent à un horodatage.
* Le beatPhase : en calculant le temps normalisé écoulé depuis le dernier horodatage de la séquence.
Cette unification représente une simplification architecturale majeure et une augmentation spectaculaire de la robustesse. Elle transforme le moteur rythmique d'un ensemble de mesures décorrélées en un modèle unique et cohérent de la structure temporelle de la musique.


3.2 Déconstruction de l'Algorithme d'Ellis (2007)


L'algorithme de programmation dynamique (DP) trouve le chemin optimal en construisant une solution de manière récursive. Il s'appuie sur trois composants principaux.1


Composant 1 : L'Enveloppe de Force d'Onset O(t)


C'est le score de correspondance locale. Il s'agit simplement de la fonction ODF robuste que nous avons conçue dans la Partie I. Pour tout instant t, la valeur O(t) indique à quel point cet instant est un bon candidat pour être un battement, en se basant uniquement sur les caractéristiques acoustiques du signal.


Composant 2 : Le Coût de Transition F(Δt,τp​)


C'est le score de régularité. Il pénalise un intervalle de temps proposé entre deux battements, Δt, s'il s'écarte de la période de battement cible, τp​, qui est dérivée de notre estimation de tempo de la Partie II. La fonction de pénalité standard, utilisée pour être symétrique aux accélérations et décélérations, est une gaussienne dans le domaine log-temporel 1 :
F(Δt,τp​)=−(log(Δt/τp​))2


Cette fonction attribue un score proche de zéro (pénalité minimale) lorsque l'intervalle Δt est très proche de la période cible τp​, et une pénalité de plus en plus négative à mesure qu'il s'en écarte.


Composant 3 : La Solution par Programmation Dynamique


Le cœur de l'algorithme est une relation de récurrence qui permet de trouver la séquence de battements avec le score total le plus élevé de manière efficace. Soit D(t) le score cumulé maximal possible pour une séquence de battements se terminant à l'instant t. On peut calculer D(t) en examinant tous les instants de battement précédents possibles τ et en choisissant celui qui maximise le score global :


D(t)=O(t)+τ<tmax​{D(τ)+α⋅F(t−τ,τp​)}


Où α est un paramètre de pondération crucial qui équilibre l'importance de la correspondance locale (coller aux onsets) par rapport à la régularité rythmique (maintenir un tempo stable).5
L'algorithme se déroule en deux passes :
1. La Passe Avant (Forward Pass) : L'algorithme parcourt le temps de t=0 jusqu'à la fin du signal (ou de la fenêtre d'analyse). À chaque pas de temps t, il calcule et mémorise D(t) (le score cumulé maximal) et B(t) (un "pointeur arrière" vers le meilleur instant de battement précédent τ qui a permis d'obtenir ce score).
2. La Passe Arrière (Backtracking) : Une fois la passe avant terminée, l'algorithme trouve l'instant tfinal​ qui a le score global D(t) le plus élevé. C'est le dernier battement de la séquence optimale. Ensuite, il suit les pointeurs arrière : le battement précédent est tpenultieme​=B(tfinal​), celui d'avant est B(tpenultieme​), et ainsi de suite, jusqu'à remonter au début du signal. La séquence d'instants ainsi collectée constitue le chemin de battements optimal.1


3.3 Guide d'Implémentation : La Solution DP à Deux Passes en TypeScript


L'implémentation de cet algorithme est la partie la plus complexe de ce guide, mais elle est réalisable en suivant rigoureusement la logique décrite.


L'Adaptation Cruciale pour le Temps Réel : La Fenêtre Glissante


L'algorithme DP, tel que décrit dans la littérature académique, est un processus hors ligne qui s'exécute sur un morceau de musique entier.5 Pour une application en temps réel comme AuraSync, cette approche n'est pas viable. Une application naïve de la logique DP trame par trame serait soit incorrecte (car elle manquerait de contexte futur pour prendre des décisions optimales), soit informatiquement prohibitive.
La solution standard, recommandée dans le rapport d'analyse, est d'appliquer l'algorithme DP sur une fenêtre glissante de l'historique de l'ODF.1
1. Maintenir un tampon des 10 à 20 dernières secondes de données ODF.
2. Exécuter périodiquement (par exemple, toutes les quelques secondes) l'algorithme DP complet (passe avant et passe arrière) sur ce tampon.
3. Les battements détectés dans la partie la plus récente de la fenêtre (par exemple, les 5 dernières secondes) sont considérés comme "confirmés" et sont utilisés pour mettre à jour l'état du visualiseur. Les battements plus anciens sont écartés.
Cette stratégie offre un excellent compromis entre la réactivité nécessaire pour le temps réel et le contexte temporel suffisant pour que l'algorithme puisse trouver un chemin globalement optimal à l'intérieur de la fenêtre. Une autre optimisation standard consiste à sous-échantillonner l'ODF (par exemple, à 50-100 Hz) avant d'exécuter la DP, ce qui réduit considérablement le temps de calcul sans perte significative de précision.1


Code Bloc 1 : La Passe Avant


Ce code implémente la passe avant de l'algorithme, en calculant les scores cumulés (D) et les pointeurs arrière (B). Il est basé sur le pseudo-code de 1 et la logique de l'algorithme d'Ellis.5


TypeScript




// Pseudo-code pour la passe avant de la programmation dynamique
// à intégrer dans la nouvelle classe RhythmicEngine

/**
* Exécute la passe avant de l'algorithme de programmation dynamique.
* @param odf Le tampon de l'historique de la Fonction de Détection d'Onset.
* @param targetPeriod La période de battement cible, en nombre d'échantillons ODF.
* @param alpha Le poids de la pénalité de transition.
* @returns Un objet contenant les scores accumulés (D) et les pointeurs arrière (B).
*/
function forwardPass(odf: Float32Array, targetPeriod: number, alpha: number): { D: Float32Array; B: Int32Array } {
   const N = odf.length;
   const D = new Float32Array(N).fill(0); // Scores accumulés
   const B = new Int32Array(N).fill(-1); // Pointeurs arrière

   // Fonction de pénalité de transition (coût de régularité)
   const penalty = (delta: number, target: number) => -Math.pow(Math.log(delta / target), 2);

   for (let t = 1; t < N; t++) {
       let bestPrevScore = -Infinity;
       let bestPrevT = -1;

       // Limiter la recherche du battement précédent à une fenêtre plausible
       // pour des raisons de performance et de pertinence musicale.
       const searchStart = Math.max(0, Math.floor(t - 2 * targetPeriod));
       const searchEnd = Math.floor(t - 0.5 * targetPeriod);

       for (let prev_t = searchStart; prev_t < searchEnd; prev_t++) {
           // Score = score cumulé du chemin précédent + pénalité pour le saut
           const score = D[prev_t] + alpha * penalty(t - prev_t, targetPeriod);
           if (score > bestPrevScore) {
               bestPrevScore = score;
               bestPrevT = prev_t;
           }
       }

       if (bestPrevT!== -1) {
           // Score total = score de l'onset local + meilleur score du chemin précédent
           D[t] = odf[t] + bestPrevScore;
           B[t] = bestPrevT;
       } else {
           // Si aucun prédécesseur plausible n'est trouvé, ce battement commence une nouvelle séquence.
           D[t] = odf[t]; 
       }
   }
   return { D, B };
}



Code Bloc 2 : La Passe Arrière


Cette fonction prend les tableaux D et B de la passe avant et reconstruit la séquence optimale de battements.


TypeScript




// Pseudo-code pour la passe arrière (backtracking)
// à intégrer dans la nouvelle classe RhythmicEngine

/**
* Reconstruit la séquence de battements optimale en suivant les pointeurs arrière.
* @param D Le tableau des scores accumulés.
* @param B Le tableau des pointeurs arrière.
* @returns Un tableau d'indices représentant les instants des battements.
*/
function backwardPass(D: Float32Array, B: Int32Array): number {
   const beats: number =;
   if (D.length === 0) return beats;

   // 1. Trouver le dernier battement : celui avec le score cumulé le plus élevé dans la fenêtre.
   let lastBeatIndex = 0;
   for (let i = 1; i < D.length; i++) {
       if (D[i] > D) {
           lastBeatIndex = i;
       }
   }

   // 2. Suivre les pointeurs arrière pour reconstruire le chemin
   if (D > 0) { // S'assurer qu'un chemin valide a été trouvé
       beats.push(lastBeatIndex);
       let currentBeatIndex = lastBeatIndex;
       while (B > 0) {
           currentBeatIndex = B;
           beats.unshift(currentBeatIndex); // Ajouter au début pour conserver l'ordre chronologique
       }
   }

   return beats;
}

L'implémentation de ce système de suivi de battement représente le plus grand saut qualitatif possible pour AuraSync. Il fournit la base nécessaire pour des animations complexes et musicalement significatives, parfaitement synchronisées avec la structure rythmique perçue de la musique, résolvant ainsi le problème fondamental identifié par l'utilisateur.
________________


Conclusion : Une Nouvelle Architecture Rythmique Modulaire pour AuraSync


L'analyse approfondie du moteur rythmique d'AuraSync a révélé un système fonctionnel mais architecturalement limité. L'implémentation actuelle, bien que compétente, souffre de faiblesses inhérentes à ses choix algorithmiques : une Fonction de Détection d'Onset (ODF) sensible au bruit et un détecteur de tempo par autocorrélation incapable de gérer l'ambiguïté musicale. Ce guide a présenté une feuille de route complète pour refondre ce moteur en s'appuyant sur des techniques robustes et validées par la communauté scientifique.


Synthèse des Améliorations


Le parcours correctif proposé en trois étapes transforme le système de manière fondamentale, chaque étape s'appuyant sur la précédente :
1. Une ODF Robuste : En remplaçant le flux spectral pleine bande par une approche multi-bandes (échelle Mel) avec agrégation par la médiane, nous avons construit une base de signal propre, résiliente aux bruits non rythmiques. C'est l'amélioration qui aura l'impact le plus significatif sur la fiabilité globale.
2. Une Induction de Tempo Multi-Hypothèses : En intégrant une analyse par DFT (tempogramme) pour générer des candidats de tempo et en utilisant l'ACF existante comme mécanisme de validation, nous avons corrigé la principale faiblesse de l'algorithme actuel : les erreurs d'octave et l'incapacité à voir plusieurs couches rythmiques.
3. Un Suivi de Battement Précis : En mettant en œuvre l'algorithme de programmation dynamique d'Ellis sur une fenêtre glissante, nous avons unifié la détection de tempo et de phase. Le système ne se contente plus d'estimer une vitesse, il fournit une séquence précise d'instants de battement, non sujette à la dérive, qui est la clé d'une synchronisation audio-visuelle de niveau professionnel.


Proposition d'Architecture : La Classe RhythmicEngine


Pour formaliser cette refonte et garantir la modularité et la maintenabilité, il est fortement recommandé d'encapsuler l'ensemble de cette nouvelle logique dans une classe dédiée, RhythmicEngine. Cette classe serait instanciée et gérée au sein du hook useAudioAnalyzer et serait responsable de l'ensemble du pipeline rythmique.
Le tableau suivant résume l'évolution architecturale entre l'implémentation actuelle et la proposition, mettant en évidence les gains fonctionnels à chaque niveau.
Caractéristique
	Implémentation Actuelle (BPMDetector.ts)
	Implémentation Proposée (RhythmicEngine)
	ODF
	Flux spectral pleine bande
	Flux spectral multi-bandes avec agrégation par médiane
	Méthode de Périodicité
	Autocorrélation (ACF) seule
	Hybride (DFT + ACF) pour des hypothèses multiples
	Sortie Principale
	Un seul BPM, souvent sujet à des erreurs d'octave
	Liste de tempos candidats + un BPM principal validé
	Gestion Erreurs d'Octave
	Aucune
	Heuristiques basées sur la comparaison des hypothèses
	Précision de la Phase
	Heuristique basée sur les transitoires (sujette à la dérive)
	Suivi précis via programmation dynamique
	Modularité
	Monolithique
	Composants distincts pour ODF, tempo et suivi de battement
	En suivant cette feuille de route, AuraSync peut transcender ses limitations actuelles. Le projet passera d'un visualiseur réagissant au rythme à un système qui comprend et anticipe la structure temporelle de la musique. Cette transformation ne se limite pas à une simple amélioration de la précision ; elle ouvre un horizon de possibilités créatives et artistiques considérablement plus vaste, permettant de réaliser des visualisations qui ne sont pas seulement synchronisées, mais véritablement musicales.
Sources des citations
1. Améliorer Détection BPM sans Packages_.pdf
2. Example of tempo calculation in Ellis' (2007a) beat-tracking algorithm.... - ResearchGate, consulté le juillet 26, 2025, https://www.researchgate.net/figure/Example-of-tempo-calculation-in-Ellis-2007a-beat-tracking-algorithm-Beats-are_fig1_285143609
3. How to Implement the FFT: A Coding Tutorial - YouTube, consulté le juillet 26, 2025, https://www.youtube.com/watch?v=isUMxOIpcj4
4. Novailoveyou/algorithm-fast-fourier-transform - GitHub, consulté le juillet 26, 2025, https://github.com/Novailoveyou/algorithm-fast-fourier-transform
5. Beat Tracking by Dynamic Programming - Columbia University, consulté le juillet 26, 2025, https://www.ee.columbia.edu/~dpwe/pubs/Ellis07-beattrack.pdf
6. Beat Tracking by Dynamic Programming, consulté le juillet 26, 2025, https://www.audiolabs-erlangen.de/resources/MIR/FMP/C6/C6S3_BeatTracking.html
7. Beat Tracking with Dynamic Programming - Columbia Academic Commons, consulté le juillet 26, 2025, https://academiccommons.columbia.edu/doi/10.7916/D8MW2SHK/download